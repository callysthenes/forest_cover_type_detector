{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml2_group_assignment.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkgreen> Group A: </font> <img src=\"Team_A.png\" width=\"800\"/>\n",
    "#### Stephanie Gessler, Pedro V. Esteban, Itay Young, Salma ElGuendy, Connie Kim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Introduction </font>\n",
    "\n",
    "This is a continuation of forest_cover_type_detector_gr_a_Part1\n",
    "\n",
    "Above we import the files created in the previous notebook so that this notebook can run independently\n",
    "\n",
    "Table of contents is an extension of the previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tree_types.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Table of contents </font>\n",
    "\n",
    "* Data Analysis\n",
    "* Exploratory Data Analysis\n",
    "* Feature Engineering & Selection\n",
    "* Compare Several Machine Learning Models\n",
    "* Perform Hyperparameter Tuning on the Best Model\n",
    "* Interpret Model Results\n",
    "* Evaluate the Best Model with Test Data (replying the initiating question)\n",
    "* Summary & Conclusions\n",
    "\n",
    "# Sections \n",
    "* [Libaries used](#0)\n",
    "* [0. Import Data](#0)\n",
    "* [5. Feature Engineering](#5.)  \n",
    "  * [5.1 Check for Anomalies and Outliers](#5.1)\n",
    "       * [5.1.1 Outlier Detection Treatment using Inter-Quartile Range rule Function](#5.1.1)\n",
    "       * [5.1.2 Inter-Quartile Range rule: 4 IQR from Median](#5.1.2)\n",
    "       * [5.1.3 Inter-Quartile Range rule: 3 IQR from Median](#5.1.3)\n",
    "  * [5.2 Feature Transformation and Building of new features](#5.2)\n",
    "      * [5.2.1 Bivariate Combinations](#5.2.1)  \n",
    "      * [5.2.2 Polynominal](#5.2.2)\n",
    "      * [5.2.3 ID](#5.2.3)  \n",
    "      * [5.2.4 Distance to Hydrology](#5.2.4)   \n",
    "      * [5.2.5 Horizontal Distance To Roadways ](#5.2.5) \n",
    "      * [5.2.6 Slope](#5.2.6)  \n",
    "      * [5.2.7 Horizontal Distance To Fire Points ](#5.2.7)  \n",
    "      * [5.2.8 Hillshade](#5.2.8) \n",
    "          * [5.2.8.1 Mean Hillshade](#5.2.8.1) \n",
    "          * [5.2.8.2 Hillshade 9am](#5.2.8.2)\n",
    "          * [5.2.8.3 Hillshade Noon](#5.2.8.3)          \n",
    "          * [5.2.8.4 Hillshade 3pm](#5.2.8.4)       \n",
    "          * [5.2.8.5 Hillshade Ratios](#5.2.8.5)        \n",
    "      * [5.2.9 Geoclimate Groping](#5.2.9) \n",
    "* [6. Feature Selection](#6)\n",
    "  * [6.0 Prepare Data and Standardization](#6.1)\n",
    "  * [6.1 Single tree](#6.2) \n",
    "  * [6.2 Bagging](#6.2) \n",
    "  * [6.3 Random Forest](#6.3)  \n",
    "  * [6.4 Extra Trees](#6.4)\n",
    "       * [6.4.1 Feature Number Selecion](#6.6.1)       \n",
    "       * [6.4.2 Lasso Regularization](#6.6.2)\n",
    "       * [6.4.3 Filter Methods](#6.6.3)   \n",
    "  * [6.5 Recursive Feature Elimination](#6.5)  \n",
    "  * [6.6 Tree Based Methodologies](#6.6) \n",
    "       * [6.6.1 RandomForestClassifier](#6.6.1)       \n",
    "       * [6.6.2 XGBoost](#6.6.2)\n",
    "       * [6.6.3 Extra trees Classifier](#6.6.3)       \n",
    "  * [6.7 Score of all methods Together](#6.7) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"roosevelt-national-forest.jpeg\" width=1200 height=800 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "# <font color=green> Libraries used </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygraphviz in /opt/anaconda3/lib/python3.9/site-packages (1.10)\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install squarify\n",
    "#!pip install htmltabletomd\n",
    "#!pip install GraphViz\n",
    "!pip install pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "import seaborn as sns  # Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify #treemap\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import plotly.graph_objects as go\n",
    "import xgboost as xgb\n",
    "import scipy.stats as stats\n",
    "import htmltabletomd\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "from yellowbrick.target import FeatureCorrelation\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.model_selection import rfecv\n",
    "\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import Image  \n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from dtreeviz.trees import *\n",
    "\n",
    "\n",
    "from numpy import percentile\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "##  <font color=green>0.Import the Data </font>\n",
    "\n",
    "Let’s load the original Kaggle training and test data and create a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the original dataset for later comparisons and make a copy for the FE process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = data_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "<a id='5.1'></a>\n",
    "# <font color=green>  5.Feature Engineering<font>\n",
    "# <font color=green>  5.1. Check for Anomalies and Outliers <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Z-score is sensitive to the  mean and standard deviation and its assumption is a normal distribution, we cannot use the z-score for outlier handling because our data is skewed and failed to pass the normal test. Our data is not normally distributed or at least not just yet.\n",
    "\n",
    "The disadvantage using percentiles is that it considers always elements at both sides of the spectrum of the lowest or highest value, which can potentially be mistaken as outliers. \n",
    "\n",
    "As the number of observations increases, so does the number of observations considered outliers. After all, using a percentile based method will always flat-out and reject a certain percentage of our observations.Thus, we need to use the percentiles with caution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.1'></a>\n",
    "### <font color=darkcyan> 5.1.1 Outlier Detection Treatment using Inter-Quartile Range rule Function <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IQR is the difference between the 75th and 25th percentile. The IQR is more resistant to outliers. The IQR by definition only covers the middle 50% of the data, so outliers are well outside this range and the presence of a small number of outliers is not likely to change this significantly. \n",
    "\n",
    "Now we are testing different ranges for IQR, namely 2,3 and 4 to check for more extreme outlier values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_function(df, col_name,value_IQR):\n",
    "    ''' This function detects first and third quartile and interquartile range for a given column\n",
    "    of a dataframe. Then calculates upper and lower limits to determine outliers conservatively and\n",
    "    returns the number of lower and uper limit and number of outliers respectively\n",
    "    '''\n",
    "    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n",
    "    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n",
    "    IQR = third_quartile - first_quartile\n",
    "                      \n",
    "    upper_limit = third_quartile+(value_IQR*IQR)\n",
    "    lower_limit = first_quartile-(value_IQR*IQR)\n",
    "    outlier_count = 0\n",
    "                      \n",
    "    for value in df[col_name].tolist():\n",
    "        if (value < lower_limit) | (value > upper_limit):\n",
    "            outlier_count +=1\n",
    "    return lower_limit, upper_limit, outlier_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.2'></a>\n",
    "### <font color=darkcyan> 5.1.2 Inter-Quartile Range rule: 4 IQR from Median <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop through all columns to see if there are any outliers, for all values which are not only 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 outliers in Vertical_Distance_To_Hydrology\n",
      "There are 1 outliers in Hillshade_9am\n"
     ]
    }
   ],
   "source": [
    "for column in [\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\"Horizontal_Distance_To_Fire_Points\"]:\n",
    "    if outlier_function(data_train, column,4)[2] > 0:\n",
    "        print(\"There are {} outliers in {}\".format(outlier_function(data_train, column,4)[2], column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 1 record of Hillshade_9am with a zero value, which is a valid value as Hillshade can be zero. This is because there are parts in the mountain that never see the sunlight (blind spots). Hence we keep the value as it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove outliers and test again in our baseline model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "q25, q75 = percentile(data_train['Vertical_Distance_To_Hydrology'], 25), percentile(data_train['Vertical_Distance_To_Hydrology'], 75)\n",
    "iqr = q75 - q25\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 4\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# remove outliers\n",
    "data_train_vd_h = data_train[(data_train['Vertical_Distance_To_Hydrology'] > lower) & (data_train['Vertical_Distance_To_Hydrology'] < upper)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking up to see if the model improves after removing vertical distance to hydrology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4=data_train_vd_h.drop(labels=['Id','Cover_Type'],axis=1)\n",
    "y4=data_train_vd_h['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X4[scale_numerical]=scaler.fit_transform(X4[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y4.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train,X4_val,y4_train,y4_val = train_test_split(X4,y4,random_state=37) #seed is 37!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_iqr4 = RandomForestClassifier(random_state=37)\n",
    "model_forest_iqr4 = forest_iqr4.fit(X4_train,y4_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "model_forest_iqr4.score(X4_val,y4_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=37)\n",
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(model_forest_iqr4, X4_val, y4_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th><b>Algorithm</b></th>\n",
    "    <th><b>Accuracy</b></th>\n",
    "    <th><b>CV Accuracy</b></th>\n",
    "    <th><b>Accuracy with IQR4</b></th>\n",
    "    <th><b>CV Accuracy with IQR4</b></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "<td> Random Forest  </td>\n",
    "    <td> <b>0.8613<b></td>\n",
    "    <th><b>0.8016</b></th>\n",
    "      <td> 0.8607</td>\n",
    "      <td> 0.7993</td>\n",
    "  </tr>     \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the previous score with the new score after removing the outliers of vertical distance to Hydrology: Accuracy decresases slightly, hence better not to remove outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.3'></a>\n",
    "### <font color=darkcyan> 5.1.3 Inter-Quartile Range rule: 3 IQR <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are checking which variables will be removed using the 3 IQR dunction Hillshade_9am,Hillshade_Noon,Hillshade_3pm has been excluded as these are valid outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 53 outliers in Horizontal_Distance_To_Hydrology\n",
      "There are 49 outliers in Vertical_Distance_To_Hydrology\n",
      "There are 3 outliers in Horizontal_Distance_To_Roadways\n",
      "There are 132 outliers in Horizontal_Distance_To_Fire_Points\n"
     ]
    }
   ],
   "source": [
    "for column in [\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Roadways\",\"Horizontal_Distance_To_Fire_Points\"]:\n",
    "    if outlier_function(data_train, column,3)[2] > 0:\n",
    "        print(\"There are {} outliers in {}\".format(outlier_function(data_train, column,3)[2], column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\"Horizontal_Distance_To_Fire_Points\"] # one or more\n",
    "\n",
    "Q1 = data_train[cols].quantile(0.25)\n",
    "Q3 = data_train[cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df = data_train[~((data_train[cols] < (Q1 - 3 * IQR)) |(data_train[cols] > (Q3 + 3 * IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14866, 56)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3=df.drop(labels=['Cover_Type'],axis=1)\n",
    "y3=df['Cover_Type']\n",
    "\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X3[scale_numerical]=scaler.fit_transform(X3[scale_numerical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the cover types you can see the values become unbalanced if we decided to remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2160\n",
       "6    2159\n",
       "3    2153\n",
       "5    2121\n",
       "1    2113\n",
       "7    2088\n",
       "2    2072\n",
       "Name: Cover_Type, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3,X_val3,y_train3,y_val3 = train_test_split (X3,y3,random_state=37) #seed is 37!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_iqr3 = RandomForestClassifier(random_state=37)\n",
    "model_forest_iqr3 = forest_iqr3.fit(X_train3,y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87409200968523"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_forest_iqr3.score(X_val3,y_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8025\n"
     ]
    }
   ],
   "source": [
    "forest_iqr3 = RandomForestClassifier(random_state=37)\n",
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(model_forest_iqr3, X_val3, y_val3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th><b>Algorithm</b></th>\n",
    "    <th><b>Accuracy Baseline</b></th>\n",
    "    <th><b>CV Accuracy Baseline</b></th>\n",
    "    <th><b>Accuracy with IQR3</b></th>\n",
    "    <th><b>CV Accuracy with IQR3</b></th>\n",
    "      </tr>\n",
    "    </tr>\n",
    "<td> Random Forest  </td>\n",
    "    <td> <b>0.8613<b></td>\n",
    "    <th><b>0.8016</b></th>\n",
    "    <td> 0.8740</td>\n",
    "    <td> 0.8025</td>\n",
    "      </tr>     \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the IQR3 rule to remove all outliers does not significantly improve the model hence we will not remove any outliers using IQR3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.3'></a>\n",
    "### <font color=darkcyan> 5.1.3 Inter-Quartile Range rule: 1.5 IQR <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are checking which variables will be removed using the 3 IQR dunction Hillshade_9am,Hillshade_Noon,Hillshade_3pm has been excluded as these are valid outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 512 outliers in Horizontal_Distance_To_Hydrology\n",
      "There are 586 outliers in Vertical_Distance_To_Hydrology\n",
      "There are 830 outliers in Horizontal_Distance_To_Roadways\n",
      "There are 645 outliers in Horizontal_Distance_To_Fire_Points\n"
     ]
    }
   ],
   "source": [
    "for column in [\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Roadways\",\"Horizontal_Distance_To_Fire_Points\"]:\n",
    "    if outlier_function(data_train, column,1.5)[2] > 0:\n",
    "        print(\"There are {} outliers in {}\".format(outlier_function(data_train, column,1.5)[2], column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\"Horizontal_Distance_To_Fire_Points\"] # one or more\n",
    "\n",
    "Q1 = data_train[cols].quantile(0.25)\n",
    "Q3 = data_train[cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df1 = data_train[~((data_train[cols] < (Q1 - 1.5 * IQR)) |(data_train[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12261, 56)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=df1.drop(labels=['Cover_Type'],axis=1)\n",
    "y1=df1['Cover_Type']\n",
    "\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X1[scale_numerical]=scaler.fit_transform(X1[scale_numerical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the cover types you can see the values become unbalanced if we decided to remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2055\n",
       "6    1912\n",
       "5    1814\n",
       "3    1780\n",
       "7    1606\n",
       "1    1555\n",
       "2    1539\n",
       "Name: Cover_Type, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1,X_val1,y_train1,y_val1 = train_test_split (X1,y1,random_state=37) #seed is 37!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_iqr1 = RandomForestClassifier(random_state=37)\n",
    "model_forest_iqr1 = forest_iqr1.fit(X_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8701891715590345"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_forest_iqr1.score(X_val1,y_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8001\n"
     ]
    }
   ],
   "source": [
    "forest_iqr1 = RandomForestClassifier(random_state=37)\n",
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(model_forest_iqr1, X_val1, y_val1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th><b>Algorithm</b></th>\n",
    "    <th><b>Accuracy Baseline</b></th>\n",
    "    <th><b>CV Accuracy Baseline</b></th>\n",
    "    <th><b>Accuracy with IQR3</b></th>\n",
    "    <th><b>CV Accuracy with IQR3</b></th>\n",
    "      </tr>\n",
    "    </tr>\n",
    "<td> Random Forest  </td>\n",
    "    <td> <b>0.8613<b></td>\n",
    "    <th><b>0.8016</b></th>\n",
    "    <td> 0.8701</td>\n",
    "    <td> 0.8001</td>\n",
    "      </tr>     \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the IQR1.5 rule to remove all outliers does not significantly improve the model hence we will not remove any outliers using IQR3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2'></a>\n",
    "## <font color=green> 5.2 Feature Transformation and Building of new features <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.1'></a>\n",
    "### <font color=green> 5.2.1 Bivariate Combinations <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During feature engineering, we want to try to create a wide variety of interactions between multiple variables in order to create new variables. \n",
    "\n",
    "\n",
    "By manipulating them together, we create opportunities to have new and impactful features which could potentially impact our target variable, thus engineering our features. \n",
    "\n",
    "For this argument, we will create as many bivariate combinations of our predicting variables using the ‘combinations’ method from itertools library.\n",
    "\n",
    "We will not make interactions with the dummy variables as these are either 0 or 1 and we will not get any additional information from making the interaction this way. \n",
    "\n",
    "Furthermore, it is not recommended to use standardization before bivariate combinations as we want to increase the signal. <br>\n",
    "\n",
    "Sources: https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755 <br>\n",
    "\n",
    "https://samchaaa.medium.com/preprocessing-why-you-should-generate-polynomial-features-first-before-standardizing-892b4326a91d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the bivariate combination we split the dataset for using it.Note this is not the split we will use later for testing the algorithm. This has only the purpose of testing all the combination and selecting the best once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and drop our target variable 'Cover_Type' from dataframe\n",
    "X = data_train.drop('Cover_Type', axis = 1)\n",
    "\n",
    "# Isolate our dependent variable as a feature\n",
    "y = data_train['Cover_Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split (80/20 size), drop duplicates and missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .2, random_state=37, stratify=y)\n",
    "\n",
    "X_train.drop_duplicates(inplace = True)\n",
    "X_train.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create every possible bivariate combination to be tested for feature engineering, no dummies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take categorical variables prior to our feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = X_train.columns\n",
    "filtered_column_list = [column for column in column_list if 'Soil_Type' not in column and 'Wilderness_Area' not in column and 'Id' not in column ] \n",
    "interactions = list(combinations(filtered_column_list, 2))\n",
    "interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition and division has been taken out as it created a lot of noise in the data. The division makes sense if it has a business meaning and the addition only if it is the same scale. \n",
    "\n",
    "However we will add the variables which have the same metrics together in a second step but not in a for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (key, value) in interactions:\n",
    "    data_train[key + '_x_' + value] = data_train[key] * data_train[value]\n",
    "    #data_train[key + '_+_' + value] = data_train[key] + data_train[value]\n",
    "    #data_train[key + '_divide_' + value] = data_train[key] / data_train[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) #to make all columns visible in dataframe now that we have many\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.2'></a>\n",
    "### <font color=green> 5.2.2 Polynomial Features <font>\n",
    "    \n",
    "We have just seen how to make two variables interact together, but sometimes the relationship between dependent and independent variables are more complex and not linear. \n",
    "    \n",
    "Polynomials is another way to create new features! A very strong option for new features is increasing the power of a single variable. \n",
    "    \n",
    "For our purposes, we will try and see if all the existing variables, can improve our Baseline by being increased to the  power.<br>\n",
    "Source: https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select only the columns we are interested in, this is from column 2 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_int_pf = data_train.iloc[:, 1:10]\n",
    "X_train_int_pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, when running polynomials we lose the name of our labels. This function gives us the ability to preserve it with the name + the transformation done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialFeatures_labeled(input_df,power):\n",
    "    '''Basically this is a cover for the sklearn preprocessing function. \n",
    "    The problem with that function is if you give it a labeled dataframe, it ouputs an unlabeled dataframe with potentially\n",
    "    a whole bunch of unlabeled columns. \n",
    "\n",
    "    Inputs:\n",
    "    input_df = Your labeled pandas dataframe (list of x's not raised to any power) \n",
    "    power = what order polynomial you want variables up to. (use the same power as you want entered into pp.PolynomialFeatures(power) directly)\n",
    "\n",
    "    Ouput:\n",
    "    Output: This function relies on the powers_ matrix which is one of the preprocessing function's outputs to create logical labels and \n",
    "    outputs a labeled pandas dataframe   \n",
    "    '''\n",
    "    poly = PolynomialFeatures(power)\n",
    "    output_nparray = poly.fit_transform(input_df)\n",
    "    powers_nparray = poly.powers_\n",
    "\n",
    "    input_feature_names = list(input_df.columns)\n",
    "    target_feature_names = [\"Constant Term\"]\n",
    "    for feature_distillation in powers_nparray[1:]:\n",
    "        intermediary_label = \"\"\n",
    "        final_label = \"\"\n",
    "        for i in range(len(input_feature_names)):\n",
    "            if feature_distillation[i] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                variable = input_feature_names[i]\n",
    "                power = feature_distillation[i]\n",
    "                intermediary_label = \"%s^%d\" % (variable,power)\n",
    "                if final_label == \"\":         #If the final label isn't yet specified\n",
    "                    final_label = intermediary_label\n",
    "                else:\n",
    "                    final_label = final_label + \" x \" + intermediary_label\n",
    "        target_feature_names.append(final_label)\n",
    "    output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n",
    "    return output_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynominal features of degree two gives us 46 new features. Since we have already enough information, we will not go for Polynominal three to avoid dimensionality issues later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_pw2 = PolynomialFeatures_labeled(X_train_int_pf,2)\n",
    "pd.set_option('display.max_columns', None)\n",
    "output_df_pw2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some fields duplicated with respect to the original df. This is a side effect of the function as normal features are replicated to the power of one which is still the same value so we delete these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = output_df_pw2.columns\n",
    "cols = [column for column in column_list if '^1' not in column]\n",
    "output_df_pw2=output_df_pw2[cols]\n",
    "output_df_pw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here, we concatenate our output to consolidate the ponlynomials with the feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.concat([data_train,output_df_pw2], axis=1)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a huge dataset. Just for curiosity we ran polynomials to the power of three and see that just the resulting dataframe is already bigger than the consolidated one above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_pw3 = PolynomialFeatures_labeled(X_train_int_pf,3)\n",
    "output_df_pw3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.3'></a>\n",
    "### <font color=green> 5.2.3 ID <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We agree for the test to not remove ID because the ID is the unique indentifier to evaluate\n",
    "\n",
    "\n",
    "For the train we will remove it as it doesn't add any value to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop('Id',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.4'></a>\n",
    "### <font color=green> 5.2.4 Distance To Hydrology <font>\n",
    "#### <font color=green> New Features <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine Vertical distance to Hydrology and Horizontal distance to Hydrology since these two are highly correlated. This suggests to attempt a diagonal distance to hidrology using the Pythagoras theorem.\n",
    "\n",
    "We will call this newly engineered feature, Distance_To_Hydrology\n",
    " \n",
    "Source : https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Distance_To_Hydrology'] = data_train['Horizontal_Distance_To_Hydrology']**2 +data_train['Vertical_Distance_To_Hydrology']**2\n",
    "data_train['Distance_To_Hydrology'] = data_train['Distance_To_Hydrology']**0.5\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are checking the distribution of the newly created variable and see if further transformation is needed. \n",
    "\n",
    "The Distance to Hydrology inherits skewness from parent variables. It is positively skewed and has zero values. \n",
    "\n",
    "In order to use log we will use log + 1 in order to use logarithm with zero values. \n",
    "\n",
    "Source: https://www.youtube.com/watch?v=_c3dVTRIK9c and \n",
    "\n",
    "Source_2: https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb, the skewness can be interpreted as follows:\n",
    "<img src=\"Skew.png\" width=400 height=200 align=\"center\">\n",
    "\n",
    "Source: https://www.marsja.se/transform-skewed-data-using-square-root-log-box-cox-methods-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Distance_To_Hydrology'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Distance_To_Hydrology'].min(),\n",
    "      \"\\nmax\\n\", data_train['Distance_To_Hydrology'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do some transformations to minimize skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the log10+ 1 logarithm \n",
    "data_train['log10_Distance_To_Hydrology'] = np.log10(data_train['Distance_To_Hydrology']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the square root \n",
    "data_train['sqr_Distance_To_Hydrology'] = data_train['Distance_To_Hydrology']**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Square root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m' +\"Skew after Log transformation\\n\", data_train['log10_Distance_To_Hydrology'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log10_Distance_To_Hydrology'].min(),\n",
    "      \"\\nmax\\n\", data_train['log10_Distance_To_Hydrology'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+ \"Skew after Square Root Transformation\\n\", data_train['sqr_Distance_To_Hydrology'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Distance_To_Hydrology'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Distance_To_Hydrology'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,15))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Distance_To_Hydrology'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['log10_Distance_To_Hydrology'], 'green')\n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['sqr_Distance_To_Hydrology'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, for distance to Hydrology the __square root__ showed a better performance in terms of skewness and is closer to a normal bell shaped than the logarithm transformation. We will be using Square Root as a new feature in the dataset and will frop the others from the dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stephanie, you removed only log10 but not distance to hydrology? Why? Aren't we suppose to just keep the sqr transformation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log10_Distance_To_Hydrology'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.5'></a>\n",
    "### <font color=green> 5.2.5 Horizontal Distance To Roadways <font>\n",
    "\n",
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For log transformation there should be no zeros, negative values and the distribution should be positive skewed( bigger than 1 is positive) hence we are using the square root as you can see for logarithm transformation below the distribution did not improve!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before Transformation\\n\", data_train['Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin before Transformation\\n\", data_train['Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax before Transformation\\n\", data_train['Horizontal_Distance_To_Roadways'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Square root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero.We are using natural log and log10\n",
    "data_train['Sqr_Horizontal_Distance_To_Roadways'] = data_train['Horizontal_Distance_To_Roadways']**0.5\n",
    "data_train['log_Horizontal_Distance_To_Roadways'] = np.log(data_train['Horizontal_Distance_To_Roadways']+1)\n",
    "data_train['log10_Horizontal_Distance_To_Roadways'] = np.log10(data_train['Horizontal_Distance_To_Roadways']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+ \"Skew after Square Root Transformation\\n\", data_train['Sqr_Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin \\n\", data_train['Sqr_Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax \\n\", data_train['Sqr_Horizontal_Distance_To_Roadways'].max(),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m' +\"Skew after log Transformation\\n\", data_train['log_Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Horizontal_Distance_To_Roadways'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+ \"Skew after log10 transformation\\n\", data_train['log10_Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin \\n\", data_train['log10_Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax \\n\", data_train['log10_Horizontal_Distance_To_Roadways'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing if the sqaure root is normally distributed and it shows it is not, however it is less skewed than before\n",
    "stats.normaltest(data_train['Sqr_Horizontal_Distance_To_Roadways'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Horizontal_Distance_To_Roadways'], 'purple')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['log_Horizontal_Distance_To_Roadways'], 'green')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log10_Horizontal_Distance_To_Roadways'], 'green')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['Sqr_Horizontal_Distance_To_Roadways'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved the best result again using square root of the Horizontal Distance to Roadways. Similarly as before, we remove failed experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Horizontal_Distance_To_Roadways','log10_Horizontal_Distance_To_Roadways'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.6'></a>\n",
    "### <font color=green> 5.2.6 Slope <font>\n",
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+ \"Skew before transformation\\n\", data_train['Slope'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Slope'].min(),\n",
    "      \"\\nmax \\n\", data_train['Slope'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Sqrare root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero\n",
    "data_train['logSlope'] = np.log(data_train['Slope']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['logSlope'].skew(), \n",
    "      \"\\nmin\\n\", data_train['logSlope'].min(),\n",
    "      \"\\nmax\\n\", data_train['logSlope'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['SqrSlope'] = data_train['Slope']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['SqrSlope'].skew(), \n",
    "      \"\\nmin\\n\", data_train['SqrSlope'].min(),\n",
    "      \"\\nmax\\n\", data_train['SqrSlope'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Slope'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['logSlope'], 'green')\n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['SqrSlope'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the skweness for the slope shows better performance when using the square root, we will transform the variable into square root as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['logSlope'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.7'></a>\n",
    "### <font color=green> 5.2.7 Horizontal Distance To Fire Points  <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Horizontal_Distance_To_Fire_Points'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Horizontal_Distance_To_Fire_Points'].min(),\n",
    "      \"\\nmax\\n\", data_train['Horizontal_Distance_To_Fire_Points'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Sqrare root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero\n",
    "data_train['log_Horizontal_Distance_To_firepoints'] = np.log(data_train['Horizontal_Distance_To_Fire_Points']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Horizontal_Distance_To_firepoints'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Horizontal_Distance_To_firepoints'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Horizontal_Distance_To_firepoints'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform with square root\n",
    "data_train['sqr_Horizontal_Distance_To_firepoints'] = data_train['Horizontal_Distance_To_Fire_Points']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Horizontal_Distance_To_firepoints'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Horizontal_Distance_To_firepoints'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Horizontal_Distance_To_firepoints'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Horizontal_Distance_To_Fire_Points'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['log_Horizontal_Distance_To_firepoints'], 'green')\n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['sqr_Horizontal_Distance_To_firepoints'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since square root transformation gives the best result in skewness, we will also use sqr for the feature variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Horizontal_Distance_To_firepoints'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8'></a>\n",
    "### <font color=green> 5.2.8 Hillshades <font>\n",
    "<a id='5.2.8.1'></a>\n",
    "### <font color=green> 5.2.8.1 Mean Hillshade <font>\n",
    "#### <font color=green> Creation of new Feature: Mean Hillshade <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take the average of Hillshades,which gives you the average light exposure of each cover type during the day\n",
    "data_train['Mean_Hillshade'] = (data_train['Hillshade_9am']+data_train['Hillshade_Noon']+data_train['Hillshade_3pm'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Itensity of the Hillshade variables in 3 bin siizes with the bin discretizer\n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "data_train['Mean_Hillshade_bin'] = est.fit_transform(data_train[['Mean_Hillshade']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[['Mean_Hillshade_bin','Mean_Hillshade']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Mean_Hillshade'] = np.log(data_train['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log10Mean_Hillshade'] = np.log10(data_train['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log10 transformation\\n\", data_train['log10Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log10Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['log10Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Mean_Hillshade'] = data_train['Mean_Hillshade']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers\n",
    "# transform training data with Boxcox\n",
    "data_train['Mean_Hillshade_boxcox'], _ = stats.boxcox(data_train['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Mean_Hillshade_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Mean_Hillshade_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Mean_Hillshade_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest(data_train['Mean_Hillshade_boxcox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Mean_Hillshade'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Mean_Hillshade'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Mean_Hillshade_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Mean_Hillshade'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution did not improve with Square Root and Logarithms Transformation. Hence we use BoxCox which improved the distribution substantially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log10Mean_Hillshade','log_Mean_Hillshade','sqr_Mean_Hillshade'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8.2'></a>\n",
    "### <font color=green> 5.2.8.2 Hillshade 9am <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Hillshade_9am'] = np.log(data_train['Hillshade_9am']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Hillshade_9am'] = data_train['Hillshade_9am']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_train['Hillshade_9am_boxcox'], lam  = stats.boxcox(data_train['Hillshade_9am']+1)\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Hillshade_9am_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_9am_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_9am_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Hillshade_9am'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Hillshade_9am'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Hillshade_9am_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Hillshade_9am'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoxCox outperforms the other two for the Hillshade 9am "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Hillshade_9am','sqr_Hillshade_9am'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8.3'></a>\n",
    "### <font color=green> 5.2.8.3 Hillshade Noon <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Hillshade_Noon'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_Noon'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_Noon'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Hillshade_Noon'] = np.log(data_train['Hillshade_Noon']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Hillshade_Noon'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Hillshade_Noon'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Hillshade_Noon'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Hillshade_Noon'] = data_train['Hillshade_Noon']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Hillshade_Noon'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Hillshade_Noon'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Hillshade_Noon'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_train['Hillshade_Noon_boxcox'], lam  = stats.boxcox(data_train['Hillshade_Noon'])\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Hillshade_Noon_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_Noon_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_Noon_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Hillshade_Noon'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Hillshade_Noon'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Hillshade_Noon_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Hillshade_Noon'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Coc is outperforming the other transformations for Hillshade Noon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Hillshade_Noon','sqr_Hillshade_Noon'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8.4'></a>\n",
    "### <font color=green> 5.2.8.4 Hillshade 3pm <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Hillshade_3pm'] = np.log(data_train['Hillshade_3pm']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Hillshade_3pm'] = data_train['Hillshade_3pm']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_train['Hillshade_3pm_boxcox'], lam  = stats.boxcox(data_train['Hillshade_3pm']+1)\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Hillshade_3pm_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_3pm_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_3pm_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Hillshade_3pm'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Hillshade_3pm'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Hillshade_3pm_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Hillshade_3pm'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Hillshade 3pm the data was not highly skwed, we either keep the original or we can use boxcox as it improved the variables as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Hillshade_3pm','sqr_Hillshade_3pm'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8.5'></a>\n",
    "### <font color=green> 5.2.8.5 Hillshades  Ratios <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['ratio_Hillshade_3pm'] = data_train['Hillshade_3pm']/255\n",
    "data_train['ratio_Hillshade_Noon'] = data_train['Hillshade_Noon']/255\n",
    "data_train['ratio_Hillshade_9am'] = data_train['Hillshade_9am']/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8.6'></a>\n",
    "### <font color=green> 5.2.8.6 Aspect <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_train['Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>and Square root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Aspect'] = data_train['Aspect']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Aspect'] = np.log(data_train['Aspect']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Aspect'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['log_Aspect'], 'green')\n",
    "#f.add_subplot(334)\n",
    "#histPlot(data_train['Hillshade_3pm_boxcox'], 'gold')                    \n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['sqr_Aspect'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For aspect square root turned out to be the best transformation in terms of skeweness. \n",
    "\n",
    "Overall, the best transformations done here are square rt, and boxcox. log transformations did not proved to be benefitial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Aspect'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we are transforming the ratios into a unit scale by dividing by its index. We do so because we think it is much easier to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['ratio_Hillshade_3pm'] = data_train['Hillshade_3pm']/255\n",
    "data_train['ratio_Hillshade_Noon'] = data_train['Hillshade_Noon']/255\n",
    "data_train['ratio_Hillshade_9am'] = data_train['Hillshade_9am']/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='5.2.8.7'></a>\n",
    "### <font color=green> 5.2.8.7 Aspect in degrees <font>\n",
    "### <font color=green> New Features  <font>\n",
    "The azimuth is the angular direction of the sun, measured from north clockwise in degrees from 0 to 360. An Azimuth of 90 degrees is east.The Cut of values will be between for instance the middle of north and east.\n",
    "    \n",
    "We make a transformation so to get dummies for Aspect ordinal values: north, south, east and west\n",
    "\n",
    "* Aspect_North: from 315 deg to 45 deg\n",
    "* Aspect_East: from 45 deg to 135 deg\n",
    "* Aspect_South: from 135 deg to 225 deg\n",
    "* Aspect_West: from 225 deg to 315 deg    \n",
    "\n",
    "<img src=\"angle_azimuth.png\" width=400 height=200 align=\"center\">\n",
    "    \n",
    "Source:https://www.pveducation.org/pvcdrom/properties-of-sunlight/azimuth-angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping Aspect in the four directions\n",
    "data_train['Aspect_North']=  np.where(((data_train['Aspect']>=0) & (data_train['Aspect']<45))|((X_train['Aspect']>=315) & (X_train['Aspect']<=360)), 1 ,0)\n",
    "data_train['Aspect_East']= np.where((data_train['Aspect']>=45) & (data_train['Aspect']<135), 1 ,0)\n",
    "data_train['Aspect_South']= np.where((data_train['Aspect']>=135) & (data_train['Aspect']<225), 1 ,0)\n",
    "data_train['Aspect_West']= np.where((data_train['Aspect']>=225) & (data_train['Aspect']<315), 1 ,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='5.2.8.8'></a>\n",
    "### <font color=green> 5.2.8.8 Elevation <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No transformation is done as it is already very symetric distributed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Elevation'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Elevation'].min(),\n",
    "      \"\\nmax\\n\", data_train['Elevation'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['binned_elevation'] = [math.floor(v/50.0) for v in data_train['Elevation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are making more features byt summing and substracing different combinations similar in terms of units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition and Substraction on the same scale\n",
    "Using for loop was giving us a bad performance hence we are using the features on the same scale which to add or substract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Road+Fire'] = data_train['Horizontal_Distance_To_Roadways'] + data_train['Horizontal_Distance_To_Fire_Points']\n",
    "data_train['Road-Fire'] = abs(data_train['Horizontal_Distance_To_Roadways'] - data_train['Horizontal_Distance_To_Fire_Points'])\n",
    "data_train['Road+Hydro'] = data_train['Horizontal_Distance_To_Roadways'] + data_train['Horizontal_Distance_To_Hydrology']\n",
    "data_train['Road-Hydro'] = abs(data_train['Horizontal_Distance_To_Roadways'] - data_train['Horizontal_Distance_To_Hydrology'])\n",
    "data_train['Hydro+Fire'] = data_train['Horizontal_Distance_To_Hydrology'] + data_train['Horizontal_Distance_To_Fire_Points']\n",
    "data_train['Hydro-Fire'] = abs(data_train['Horizontal_Distance_To_Hydrology'] - data_train['Horizontal_Distance_To_Fire_Points'])\n",
    "\n",
    "data_train['Road+Fire+Hydro'] = data_train['Horizontal_Distance_To_Roadways']  + data_train['Horizontal_Distance_To_Fire_Points'] + data_train['Horizontal_Distance_To_Hydrology']\n",
    "\n",
    "data_train['Ele+Road+Fire+Hydro'] = data_train['Elevation'] + data_train['Horizontal_Distance_To_Roadways']  + data_train['Horizontal_Distance_To_Fire_Points'] + data_train['Horizontal_Distance_To_Hydrology']\n",
    "\n",
    "data_train['Ele+road'] = data_train['Elevation'] + data_train['Horizontal_Distance_To_Roadways']\n",
    "data_train['Ele-road'] = abs(data_train['Elevation'] - data_train['Horizontal_Distance_To_Roadways'])\n",
    "data_train['Ele+fire'] = data_train['Elevation'] + data_train['Horizontal_Distance_To_Fire_Points']\n",
    "data_train['Ele-fire'] = abs(data_train['Elevation'] - data_train['Horizontal_Distance_To_Fire_Points'])\n",
    "data_train['Ele+hydro'] = data_train['Elevation'] + data_train['Horizontal_Distance_To_Hydrology']\n",
    "data_train['Ele-hydro'] = abs(data_train['Elevation'] - data_train['Horizontal_Distance_To_Hydrology'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.9 Geoclimate grouping  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.1 Climatic feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Kaggle competition, there is a reference to John A. Blackard which happened to be one geologist working for the forest federal US agency. In a co-authored paper, he gives further insights on the soil families with a list of codes. These are digits categorizing the soils according to climate and geology. We decide to take this valuable insight and engineer features around this dynamic so to cut down the number of soils\n",
    "\n",
    "From original database donated by John A. Blackard\n",
    "\n",
    "Code Designations:\n",
    "\n",
    "Wilderness Areas:  \t<br>\n",
    "\n",
    "1 - Rawah Wilderness Area <br>\n",
    "2 - Neota Wilderness Area  <br>\n",
    "3 - Comanche Peak Wilderness Area<br>\n",
    "4 - Cache la Poudre Wilderness Area<br>\n",
    "\n",
    "Soil Types:             1 to 40 : based on the USFS Ecological\n",
    "                        Landtype Units (ELUs) for this study area:<br>\n",
    "\n",
    "  Study Code USFS ELU Code\t\t\tDescription<br>\n",
    "\t 1\t   2702\t\tCathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "\t 2\t   2703\t\tVanet - Ratake families complex, very stony.<br>\n",
    "\t 3\t   2704\t\tHaploborolis - Rock outcrop complex, rubbly.<br>\n",
    "\t 4\t   2705\t\tRatake family - Rock outcrop complex, rubbly.<br>\n",
    "\t 5\t   2706\t\tVanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "\t 6\t   2717\t\tVanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "\t 7\t   3501\t\tGothic family.<br>\n",
    "\t 8\t   3502\t\tSupervisor - Limber families complex.<br>\n",
    "\t 9\t   4201\t\tTroutville family, very stony.<br>\n",
    "\t10\t   4703\t\tBullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "\t11\t   4704\t\tBullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "\t12\t   4744\t\tLegault family - Rock land complex, stony.<br>\n",
    "\t13\t   4758\t\tCatamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "\t14\t   5101\t\tPachic Argiborolis - Aquolis complex.<br>\n",
    "\t15\t   5151\t\tunspecified in the USFS Soil and ELU Survey.<br>\n",
    "\t16\t   6101\t\tCryaquolis - Cryoborolis complex.<br>\n",
    "\t17\t   6102\t\tGateview family - Cryaquolis complex.<br>\n",
    "\t18\t   6731\t\tRogert family, very stony.<br>\n",
    "\t19\t   7101\t\tTypic Cryaquolis - Borohemists complex.<br>\n",
    "\t20\t   7102\t\tTypic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "\t21\t   7103\t\tTypic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "\t22\t   7201\t\tLeighcan family, till substratum, extremely bouldery.<br>\n",
    "\t23\t   7202\t\tLeighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "\t24\t   7700\t\tLeighcan family, extremely stony.<br>\n",
    "\t25\t   7701\t\tLeighcan family, warm, extremely stony.<br>\n",
    "\t26\t   7702\t\tGranile - Catamount families complex, very stony.<br>\n",
    "\t27\t   7709\t\tLeighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "\t28\t   7710\t\tLeighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "\t29\t   7745\t\tComo - Legault families complex, extremely stony.<br>\n",
    "\t30\t   7746\t\tComo family - Rock land - Legault family complex, extremely stony.<br>\n",
    "\t31\t   7755\t\tLeighcan - Catamount families complex, extremely stony.<br>\n",
    "\t32\t   7756\t\tCatamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "\t33\t   7757\t\tLeighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "\t34\t   7790\t\tCryorthents - Rock land complex, extremely stony.<br>\n",
    "\t35\t   8703\t\tCryumbrepts - Rock outcrop - Cryaquepts complex.<br>\n",
    "\t36\t   8707\t\tBross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "\t37\t   8708\t\tRock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "\t38\t   8771\t\tLeighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "\t39\t   8772\t\tMoran family - Cryorthents - Leighcan family complex, extremely <br>stony.\n",
    "\t40\t   8776\t\tMoran family - Cryorthents - Rock land complex, extremely stony.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Note:   First digit:  climatic zone       Second digit:  geologic zones\n",
    "                1.  lower montane dry             1.  alluvium\n",
    "                2.  lower montane                 2.  glacial\n",
    "                3.  montane dry                   3.  shale\n",
    "                4.  montane                       4.  sandstone\n",
    "                5.  montane dry and montane       5.  mixed sedimentary\n",
    "                6.  montane and subalpine         6.unspecified in the USFS ELU Survey\n",
    "                7.  subalpine                     7.  igneous and metamorphic\n",
    "                8.  alpine                        8.  volcanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The USFD, an American federal agency for forest service dependent on the department of agriculture has classified soil types according to __climatic zone (first digit)__ and __geology (second digit)__. Because of this, we believe a similar classification can be artificially engineered grouping all similar soils in 7 categories for climate (there is no lower montane dry soils) and 4 for geology (we do not take into consideration shale, sandstone, volcanic or unspecified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.2 Climatic Zone feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"Lower_Montane_Climate\"] = data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[23456]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montane_Dry_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[78]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montane_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][0123]$|Soil_Type[9]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montane_Dry_and_Montane_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][45]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montante_and_Subalpine_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][678]$\")].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Subalpine_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type19$|^Soil_Type[2][0-9]$|^Soil_Type[3][0-4]$\")].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Alpine_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[3][56789]$|Soil_Type40\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.2 Geological feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Note:   First digit:  climatic zone             Second digit:  geologic zones\n",
    "                1.  lower montane dry                   1.  alluvium\n",
    "                2.  lower montane                       2.  glacial\n",
    "                3.  montane dry                         3.  shale\n",
    "                4.  montane                             4.  sandstone\n",
    "                5.  montane dry and montane             5.  mixed sedimentary\n",
    "                6.  montane and subalpine               6.  unspecified in the USFS ELU Survey\n",
    "                7.  subalpine                           7.  igneous and metamorphic\n",
    "                8.  alpine                              8.  volcanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Alluvium_Soil'] = data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][45679]$|^Soil_Type[2][01]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Glacial_Soil'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[9]$|^Soil_Type[2][23]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Mixed_Sedimentary_Soil'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[7-8]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Igneus_and_Metamorphic_Soil'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1-6]$|^Soil_Type[1][01238]$|^Soil_Type[3-4]\\d$|^Soil_Type[2][4-9]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the medium paper \"Preprocessing: Why you should generate polynominal features first before standardizing\" mention it is not good practice to standardize the variablesbefore before PolynominalFeatures. This should be done after to not loss the signal of the variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and drop our target variable 'Cover_Type' from dataframe, isolating our independent variables\n",
    "X = data_train.drop('Cover_Type', axis = 1)\n",
    "\n",
    "# Isolate our dependent variable as a feature\n",
    "y = data_train['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split (80/20 size), drop duplicates and missing values\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .2, random_state = 37, stratify=y)\n",
    "\n",
    "X_train.drop_duplicates(inplace = True)\n",
    "X_train.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Factorization\n",
    "\n",
    "The numerical values present a level of detail that may be much more fine-grained than we need. For instance, the soil level can be represented by different categories (soil family, complex or stony/rubberly). We aggregate the data up which can help to avoid overfitting when the data is more aggregate.\n",
    "\n",
    "__We played with these features but reached a final conclusion of not to considering any more families. For purposes of the assignment we show the code as a RAWNBCONVERT__\n",
    "\n",
    "The only family grouping we do on soils are the ones above, on geological and climate grounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.6 Soil Type Family  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Discretization to bin the soil variable to the family type.<br>\n",
    "\n",
    "__Cathedral__ <br>\n",
    "1 Cathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Ratake__ <br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "4 Ratake family - Rock outcrop complex, rubbly.<br>\n",
    "\n",
    "__Vanet__<br>\n",
    "5 Vanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "\n",
    "__Wetmore__<br>\n",
    "6 Vanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "\n",
    "__Gothic__<br>\n",
    "7 Gothic family.<br>\n",
    "                    \n",
    "__Limber__ <br>\n",
    "8 Supervisor - Limber families complex. <br>\n",
    "\n",
    "__Troutville__<br>\n",
    "9 Troutville family, very stony.<br>\n",
    "\n",
    "__Legault__<br>\n",
    "12 Legault family - Rock land complex, stony.<br>\n",
    "29 Como - Legault families complex, extremely stony.<br>\n",
    "\n",
    "__Gateview__ <br>\n",
    "17 Gateview family - Cryaquolis complex.<br>\n",
    "\n",
    "__Rogert__<br>\n",
    "18 Rogert family, very stony.<br>\n",
    "\n",
    "\n",
    "__Como__<br>\n",
    "30 Como family - Rock land - Legault family complex, extremely stony.<br>\n",
    "\n",
    "__Bross__<br>\n",
    "36 Bross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "\n",
    "\n",
    "\n",
    "__Catamount__<br>\n",
    "10 Bullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "11 Bullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "13 Catamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "26 Granile - Catamount families complex, very stony.<br>\n",
    "32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "31 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Leighcan__<br>\n",
    "21 Typic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "22 Leighcan family, till substratum, extremely bouldery.<br>\n",
    "23 Leighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "24 Leighcan family, extremely stony.<br>\n",
    "25 Leighcan family, warm, extremely stony.<br>\n",
    "27 Leighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "28 Leighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Moran__<br>\n",
    "38 Leighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "39 Moran family - Cryorthents - Leighcan family complex, extremely stony.<br>\n",
    "40 Moran family - Cryorthents - Rock land complex, extremely stony.<br>\n",
    "\n",
    "__Others__<br> \n",
    "3 Haploborolis - Rock outcrop complex, rubbly.<br>\n",
    "15 unspecified in the USFS Soil and ELU Survey.<br>\n",
    "37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "34 Cryorthents - Rock land complex, extremely stony.<br>\n",
    "35 Cryumbrepts - Rock outcrop - Cryaquepts complex.<br>\n",
    "20 Typic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "14 Pachic Argiborolis - Aquolis complex.<br>\n",
    "16 Cryaquolis - Cryoborolis complex.<br>\n",
    "19 Typic Cryaquolis - Borohemists complex.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOT USED__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Soil Type\n",
    "family_soil_types = {\n",
    "    'Family_Cathedral': ['Soil_Type1'],\n",
    "    'Family_Retake': ['Soil_Type2', 'Soil_Type4'],\n",
    "    'Family_Vanet': ['Soil_Type5'],\n",
    "    'Family_Wetmore': ['Soil_Type6'],\n",
    "    'Family_Gothic': ['Soil_Type7'],\n",
    "    'Family_Limber': ['Soil_Type8'],\n",
    "    'Family_Troutville_': ['Soil_Type9'],\n",
    "    'Family_Legault': ['Soil_Type12', 'Soil_Type29'],\n",
    "    'Family_Gateview': ['Soil_Type17'],\n",
    "    'Family_Rogert': ['Soil_Type18'],\n",
    "    'Family_Como': ['Soil_Type30'],\n",
    "    'Family_Bross': ['Soil_Type36'],\n",
    "    'Family_Catamount': ['Soil_Type10','Soil_Type11','Soil_Type13','Soil_Type26','Soil_Type32','Soil_Type31','Soil_Type33'],\n",
    "    'Family_Leighcan': ['Soil_Type21','Soil_Type22','Soil_Type23','Soil_Type24','Soil_Type25','Soil_Type27','Soil_Type28'],\n",
    "    'Family_Moran': ['Soil_Type38','Soil_Type39','Soil_Type40'],\n",
    "    'Family_Others': ['Soil_Type3','Soil_Type15','Soil_Type37','Soil_Type34','Soil_Type35','Soil_Type20','Soil_Type14','Soil_Type16','Soil_Type19'],\n",
    "} \n",
    "\n",
    "for family in family_soil_types:\n",
    "    data_train[family] = 0\n",
    "    soil_types = family_soil_types[family]\n",
    "    for soil_type in soil_types:\n",
    "        data_train[family] += data_train[soil_type]\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will group the soil types according to their family and according to the complex and stonyness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex Group <br>\n",
    "__Rock_outcrop_complex__ <br>\n",
    "1 Cathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "3 Haploborolis - Rock outcrop complex, rubbly.<br>\n",
    "4 Ratake family - Rock outcrop complex, rubbly.<br>\n",
    "5 Vanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "6 Vanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "10 Bullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "27 Leighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "28 Leighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Ratake_families_complex__<br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "\n",
    "\n",
    "__Limber families complex__<br>\n",
    "8 Supervisor - Limber families complex.<br>\n",
    "\n",
    "__rock land complex__<br>\n",
    "11 Bullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "12 Legault family - Rock land complex, stony.<br>\n",
    "34 Cryorthents - Rock land complex, extremely stony.<br>\n",
    "40 Moran family - Cryorthents - Rock land complex, extremely stony.<br>\n",
    "\n",
    "__Cryoborolis complex__<br>\n",
    "16 Cryaquolis - Cryoborolis complex.<br>\n",
    "17 Gateview family - Cryaquolis complex.<br>\n",
    "\n",
    "__Bullwark family complex__<br>\n",
    "13 Catamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "\n",
    "__Aquolis complex__<br>\n",
    "14 Pachic Argiborolis - Aquolis complex.<br>\n",
    "\n",
    "__Borohemists complex__<br>\n",
    "19 Typic Cryaquolis - Borohemists complex.<br>\n",
    "\n",
    "__Cryaquolls complex__<br>\n",
    "20 Typic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "23 Leighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "38 Leighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "\n",
    "__till substratum complex__<br>\n",
    "21 Typic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "\n",
    "__Catamount families complex__<br>\n",
    "26 Granile - Catamount families complex, very stony.<br>\n",
    "1 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "31 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "\n",
    "__Legault families complex__<br>\n",
    "29 Como - Legault families complex, extremely stony.<br>\n",
    "30 Como family - Rock land - Legault family complex, extremely stony.<br>\n",
    "\n",
    "__Leighcan family complex__<br>\n",
    "32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "39 Moran family - Cryorthents - Leighcan family complex, extremely stony.<br>\n",
    "\n",
    "__Cryaquepts complex__<br>\n",
    "35 Cryumbrepts - Rock outcrop - Cryaquepts complex.<br>\n",
    "\n",
    "__Cryumbrepts complex__<br>\n",
    "36 Bross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "\n",
    "__Cryorthents complex__<br>\n",
    "37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "\n",
    "__others__ <br>\n",
    "7 Gothic family.<br>\n",
    "9 Troutville family, very stony.<br>\n",
    "22 Leighcan family, till substratum, extremely bouldery.<br>\n",
    "24 Leighcan family, extremely stony.<br>\n",
    "25 Leighcan family, warm, extremely stony.<br>\n",
    "18 Rogert family, very stony.<br>\n",
    "15 unspecified in the USFS Soil and ELU Survey.<br>\n",
    "\n",
    "\n",
    "Source: https://www.kaggle.com/competitions/forest-cover-type-prediction/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOT USED__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Complex Type\n",
    "family_complex_types = {\n",
    "    'Rock_outcrop_complex': ['Soil_Type1','Soil_Type2','Soil_Type3','Soil_Type4','Soil_Type5','Soil_Type6','Soil_Type10','Soil_Type27','Soil_Type28','Soil_Type33'],\n",
    "    'Ratake_families_complex': ['Soil_Type2'],\n",
    "    'Limber_families_complex': ['Soil_Type8'],\n",
    "    'Rock_land_complex': ['Soil_Type11','Soil_Type12','Soil_Type34','Soil_Type40'],\n",
    "    'Cryoborolis_complex': ['Soil_Type16','Soil_Type17'],\n",
    "    'Bullwark_family_complex': ['Soil_Type13'],\n",
    "    'Aquolis_complex_': ['Soil_Type14'],\n",
    "    'Borohemists_complex': ['Soil_Type19'],\n",
    "    'Cryaquolls_complex': ['Soil_Type20','Soil_Type23','Soil_Type38'],\n",
    "    'Till_substratum_complex': ['Soil_Type21'],\n",
    "    'Catamount_families_complex': ['Soil_Type26','Soil_Type1','Soil_Type31'],\n",
    "    'Legault_families_complex': ['Soil_Type39','Soil_Type30'],\n",
    "    'Leighcan_family_complex': ['Soil_Type32','Soil_Type39'],\n",
    "    'Cryaquepts_complex': ['Soil_Type35'],\n",
    "    'Cryumbrepts_complex': ['Soil_Type36'],\n",
    "    'Cryorthents_complex': ['Soil_Type37'],\n",
    "    'others_complex': ['Soil_Type7','Soil_Type9','Soil_Type22','Soil_Type24','Soil_Type25','Soil_Type18','Soil_Type15'],\n",
    "} \n",
    "\n",
    "for family in family_complex_types:\n",
    "    data_train[family] = 0\n",
    "    complex_types = family_complex_types[family]\n",
    "    for complex_type in complex_types:\n",
    "        data_train[family] += data_train[complex_type]\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stony__ <br>\n",
    "1 Cathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "6 Vanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "9 Troutville family, very stony.<br>\n",
    "12 Legault family - Rock land complex, stony.<br>\n",
    "18 Rogert family, very stony.<br>\n",
    "24 Leighcan family, extremely stony.<br>\n",
    "25 Leighcan family, warm, extremely stony.<br>\n",
    "26 Granile - Catamount families complex, very stony.<br>\n",
    "27 Leighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "28 Leighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "29 Como - Legault families complex, extremely stony.<br>\n",
    "30 Como family - Rock land - Legault family complex, extremely stony.<br>\n",
    "31 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "34 Cryorthents - Rock land complex, extremely stony.<br>\n",
    "36 Bross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "38 Leighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "39 Moran family - Cryorthents - Leighcan family complex, extremely stony.<br>\n",
    "40 Moran family - Cryorthents - Rock land complex, extremely stony.<br>\n",
    "\n",
    "__Rubbly__<br>\n",
    "3 Haploborolis - Rock outcrop complex, rubbly.<br>\n",
    "4 Ratake family - Rock outcrop complex, rubbly.<br>\n",
    "5 Vanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "10 Bullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "11 Bullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "13 Catamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "\n",
    "__others__<br>\n",
    "7 Gothic family.<br>\n",
    "8 Supervisor - Limber families complex.<br>\n",
    "14 Pachic Argiborolis - Aquolis complex.<br>\n",
    "15 unspecified in the USFS Soil and ELU Survey.<br>\n",
    "16 Cryaquolis - Cryoborolis complex.<br>\n",
    "17 Gateview family - Cryaquolis complex.<br>\n",
    "19 Typic Cryaquolis - Borohemists complex.<br>\n",
    "20 Typic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "21 Typic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "22 Leighcan family, till substratum, extremely bouldery.<br>\n",
    "23 Leighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "35 Cryumbrepts - Rock outcrop - Cryaquepts complex.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOT USED__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soil Type\n",
    "family_types = {\n",
    "    'Type_Stony': ['Soil_Type1','Soil_Type2', 'Soil_Type6', 'Soil_Type9', 'Soil_Type12', 'Soil_Type18', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40'],\n",
    "    'Type_Rubbly': ['Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type10', 'Soil_Type11', 'Soil_Type13'],\n",
    "    'Type_Other': ['Soil_Type7','Soil_Type8', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type35']\n",
    "} \n",
    "\n",
    "for family in family_types:\n",
    "    data_train[family] = 0\n",
    "    soil_types = family_types[family]\n",
    "    for soil_type in soil_types:\n",
    "        data_train[family] += data_train[soil_type]\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Soil type is a single variable which has been one-hot encoded presumably , so we will reverse engineer the soil type. We will eventually drop the original soil type columns which has the added effect of significantly reducing the total number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original soil features\n",
    "soil_features = [f'Soil_Type{i}' for i in range(1,41)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original soil features\n",
    "data_train.drop(columns = soil_features, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if elevation makes a difference to take out with the new interaction model improves\n",
    "data_train = data_train.drop(['Elevation^2'], axis = 1)\n",
    "data_train = data_train.drop(['Elevation'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the original scaled variables did not improve nor worsen the model. Since it does not change much the score, we remove it as we have it double in the model with the scaled features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train= data_train.drop(['Aspect','Slope','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_drop =data_train.drop(labels=[\"Cover_Type\"],axis=1)\n",
    "y_drop =data_train ['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_drop,X_val_drop,y_train_drop,y_val_drop = train_test_split (X_drop,y_drop,random_state=37) #seed is 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_dropped_variables = RandomForestClassifier(random_state=37)\n",
    "model_dropped_variables = forest_dropped_variables.fit(X_train_drop,y_train_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "model_dropped_variables.score(X_val_drop,y_val_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_dropped_variables = RandomForestClassifier(random_state=37)\n",
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(model_dropped_variables, X_val_drop, y_val_drop))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model does not improve with respect to baseline's 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.10 Summary <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th><b>Features</b></th>\n",
    "    <th><b>Transformation</b></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "<td>ID  </td>\n",
    "    <td> Drop</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Distance To Hydrology  </td>\n",
    "    <td><b><i>Square Root</i></b> of the length of the side of horizontal and vertical </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Horizontal Distance To Roadways</td>\n",
    "    <td><b>Square Root</b> of horizontal Distance to Roadways</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Slope</td>\n",
    "    <td><b><i>Square Root</i></b> Slope</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Horizontal_Distance To firepoints</td>\n",
    "    <td><b><i>Square Root</i></b> Horizontal Distance to firepoints</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mean Hillshade</td>\n",
    "    <td><b><i>Box Cox Average</i></b> of all Hillshades features</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Hillshade 9am</td>\n",
    "    <td><b><i>Box Cox </i></b> Hillshade 9am</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Hillshade Noon</td>\n",
    "    <td><b><i>Box Cox </i></b> Hillshade Noon</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Hillshade 3pm</td>\n",
    "    <td><b><i>Box Cox</i></b> Hillshade 3pm</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "        <td>Aspect</td>\n",
    "    <td><b><i>Square Root</i></b> Aspect</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Aspect North, East,South and West</td>\n",
    "    <td><b><i>Grouping</i></b> Aspect</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Geological Grouping</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Types</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Climate Grouping</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Types</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "     <td>Soil Family</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Families</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "     <td>Soil Type Complex</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Complex</td>\n",
    "  <tr>      \n",
    "  </tr> \n",
    "     <td>Soil Type Stonyness</td>\n",
    "    <td><b><i>Grouping</i></b> by Soil stonyness</td>\n",
    "  <tr>      \n",
    "  </tr>     \n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "# <font color=green> 6.Feature Selection <font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to use several feature selection algorithms where we use them in combination of all the different selection method and will take the best score of all the used common algorithms score. \n",
    "\n",
    "Source: https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locking all features in a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the shake of efficiency, we create a csv file to reuse also later on in part III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below have all features that we did in the feature engineering part and transport this to csv before we actually use the feature selection methods and select only few variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only X_Train replacement\n",
    "data_train.to_csv('all_features_data_train.csv')\n",
    "data_train.to_csv('all_features_data_train.csv') #this one is doubled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.1'></a>\n",
    "## <font color=green> 6.1. Standardization <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to do some standardization in the hopes it will help assist feature selection algorithms more efficiently.\n",
    "\n",
    "We attempt also to run a few correlation matrices and take out heavily correlated values but this did not improved baselines' accuracy so it is marked as RAWNBCONVERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOT USED__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#obtain the correlations of each features in dataset\n",
    "corrmat = data_train.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(data_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can barely see anything but I'd remove all green blocks (polynomial features)\n",
    "\n",
    "__NOT USED__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#if run twice, error will appear as labels are already deleted!\n",
    "data_train = data_train.drop([\"Horizontal_Distance_To_Hydrology_x_Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Hydrology_x_Horizontal_Distance_To_Roadways\", \"Horizontal_Distance_To_Hydrology_x_Hillshade_9am\", \"Horizontal_Distance_To_Hydrology_x_Hillshade_Noon\", \"Horizontal_Distance_To_Hydrology_x_Hillshade_3pm\", \"Horizontal_Distance_To_Hydrology_x_Horizontal_Distance_To_Fire_Points\", \"Vertical_Distance_To_Hydrology_x_Horizontal_Distance_To_Roadways\"], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#obtain the correlations of each features in dataset\n",
    "corrmat = data_train.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(data_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Green correlations are still there, I do another drop\n",
    "\n",
    "__NOT USED__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#if run twice, error will appear as labels are already deleted!\n",
    "data_train = data_train.drop([\"Horizontal_Distance_To_Roadways_x_Hillshade_9am\", \"Horizontal_Distance_To_Roadways_x_Hillshade_Noon\", \"Horizontal_Distance_To_Roadways_x_Hillshade_3pm\", \"Horizontal_Distance_To_Roadways_x_Horizontal_Distance_To_Fire_Points\", \"Vertical_Distance_To_Hydrology_x_Hillshade_9am\", \"Vertical_Distance_To_Hydrology_x_Hillshade_Noon\", \"Vertical_Distance_To_Hydrology_x_Hillshade_3pm\"], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#obtain the correlations of each features in dataset\n",
    "corrmat = data_train.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(data_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing an extra round of drops\n",
    "\n",
    "__NOT USED__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_train = data_train.drop([\"Aspect_x_Hillshade_9am\", \"Aspect_x_Hillshade_Noon\", \"Aspect_x_Hillshade_3pm\", \"ratio_Hillshade_3pm\"], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#obtain the correlations of each features in dataset\n",
    "corrmat = data_train.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(data_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now removing perfect colinearity 1s\n",
    "\n",
    "__NOT USED__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_train = data_train.drop([\"binned_elevation\", \"Mixed_Sedimentary_Soil\", \"Elevation^2\"], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#obtain the correlations of each features in dataset\n",
    "corrmat = data_train.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(data_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_train = data_train.drop([\"Type_Other\"], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_train = data_train.drop([\"Hillshade_Noon^2\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset to train and validation set, in order to test our models. We use stratify to have a balanced dataset with regards to y predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train.drop(['Cover_Type'], axis=1)\n",
    "y = data_train['Cover_Type']\n",
    "column_list = X.columns\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=37,stratify=y)\n",
    "print(\"The shape of validation data:{} and {} \".format(X_val.shape,y_val.shape))\n",
    "print(\"The shape of training data:{} and {} \".format(X_train.shape,y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the standardization we need only numerical values, since these has been aleady encoded we use the names to filter out the dummy variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_numerical  = [column for column in column_list if 'Soil' not in column and 'Wilderness_Area' not in  column and 'Aspect_North' not in  column and 'Climate' not in  column and 'Family' not in  column and 'Type' not in  column and 'complex' not in  column and 'Aspect_East' not in  column and 'Aspect_South' not in  column and 'Aspect_West' not in  column ]\n",
    "scale_categorial= [column for column in column_list if column not in scale_numerical ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see only the dummy variables filtered \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_train = data_train.filter(items=scale_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorial_train = data_train.filter(items=scale_categorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed now to standardize using the Standard Scaler after trying MinMax, which didn't bring good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[scale_numerical] = scaler.fit_transform(X_train[scale_numerical])\n",
    "X_val[scale_numerical] = scaler.fit_transform(X_val[scale_numerical])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stephanie, I passed part 3 (the decision tree and bagging regressors for feature importances) into here because this is about feature importances, just to be consistent :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.2'></a>\n",
    "## <font color=green> 6.2. Single Tree <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a basic tree regressor to find the best features in the dataset.\n",
    "\n",
    "Function below is to map the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(tree, feature_names):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(tree, out_file=dot_data, feature_names=feature_names, filled=True, rounded=True,special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    return Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tree = DecisionTreeRegressor(random_state=37)\n",
    "model_tree = single_tree.fit(X_train, y_train)\n",
    "print(\"MSE = {0:.4f}\".format(-np.mean(cross_val_score(single_tree, X_train, y_train, scoring='neg_mean_squared_error'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(model_tree, X_train_new.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree is massive, needs some prunning! Let's see the initial features selected by this algorhithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,40))\n",
    "plt.bar(X_train.columns, single_tree.feature_importances_)\n",
    "plt.title('Feature Importance', fontsize=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prune the tree by setting a max_septh which we will find with the help of GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': range(1,30)}\n",
    "\n",
    "single_tree_a = GridSearchCV(single_tree,\n",
    "                            param_grid,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "single_tree_a.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(single_tree_a.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = single_tree_a.cv_results_['mean_test_score']\n",
    "stds = single_tree_a.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, single_tree_a.cv_results_['params']):\n",
    "    print(\"MSE = %0.3f (+/-%0.03f) for %r\" % (-mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.errorbar(range(1,30,1), [-m for m in means], yerr=stds, fmt='-o')\n",
    "plt.title('MSE for different Depths', fontsize=20)\n",
    "plt.xlabel(\"Depth\", fontsize=16)\n",
    "plt.ylabel(\"MSE\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best depth to select is 12, lets plug it in the regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tree_pruned = DecisionTreeRegressor(random_state=18, max_depth=12)\n",
    "model_tree_pruned = single_tree_pruned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"MSE = {0:.4f}\".format(-np.mean(cross_val_score(single_tree_pruned, X_train, y_train, scoring='neg_mean_squared_error'))))\n",
    "print(\"Accuracy Train= {0:.4f}\".format(np.mean(cross_val_score(single_tree_pruned, X_train, y_train))))\n",
    "print(\"Accuracy Test= {0:.4f}\".format(np.mean(cross_val_score(single_tree_pruned, X_val, y_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores are not so good, lets try the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tree_pruned = DecisionTreeClassifier(random_state=18, max_depth=12)\n",
    "\n",
    "print(\"MSE = {0:.4f}\".format(-np.mean(cross_val_score(single_tree_pruned, X_train, y_train, scoring='neg_mean_squared_error'))))\n",
    "print(\"Accuracy Train= {0:.4f}\".format(np.mean(cross_val_score(single_tree_pruned, X_train, y_train))))\n",
    "print(\"Accuracy Test= {0:.4f}\".format(np.mean(cross_val_score(single_tree_pruned, X_val, y_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,40))\n",
    "plt.bar(X_train.columns, model_tree_pruned.feature_importances_)\n",
    "plt.title('Feature Importance', fontsize=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.3'></a>\n",
    "## <font color=green> 6.3. Bagging <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_bagging = RandomForestRegressor(random_state=37, n_jobs=-1, max_features=len(X_train.columns))\n",
    "print(\"MSE = {0:.4f}\".format(-np.mean(cross_val_score(tree_bagging, X_train, y_train, scoring='neg_mean_squared_error'))))\n",
    "print(\"Accuracy Train= {0:.4f}\".format(np.mean(cross_val_score(tree_bagging, X_train, y_train))))\n",
    "print(\"Accuracy Train= {0:.4f}\".format(np.mean(cross_val_score(tree_bagging, X_val, y_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,60))\n",
    "tree_bagging.fit(X_train,y_train)\n",
    "plt.bar(X_train.columns, tree_bagging.feature_importances_)\n",
    "plt.title('Feature Importance', fontsize=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image must be amplified to see the most relevant features, best to download and oped in a separate image viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.3'></a>\n",
    "## <font color=green> 6.3. Random Forest <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(random_state=37, max_features='sqrt')\n",
    "forest_fit = random_forest.fit(X_train,y_train)\n",
    "print(\"MSE = {0:.4f}\".format(-np.mean(cross_val_score(random_forest, X_train, y_train, scoring='neg_mean_squared_error'))))\n",
    "print(\"Accuracy Train= {0:.4f}\".format(np.mean(cross_val_score(random_forest, X_train, y_train))))\n",
    "print(\"Accuracy Test= {0:.4f}\".format(np.mean(cross_val_score(random_forest, X_val, y_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,60))\n",
    "plt.bar(X_train.columns, random_forest.feature_importances_)\n",
    "plt.title('Feature Importance', fontsize=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is almost an scaled version of the bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.4'></a>\n",
    "## <font color=green> 6.4. Extra Trees <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ran Feature Importance Strategies, first we do the extra trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stephanie, can you please check if the Classifier is the way to go for the Extra Trees and not the Regressor? For feature importances I always used regressor and for the model the classifier (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) Plus, we need CV as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_) \n",
    " \n",
    "#plot the graph of feature importances \n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "ax = feat_importances.nlargest(40).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting features seem to be reasonable to use, elevation being the most important one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.4.1'></a>\n",
    "## <font color=green> 6.4.1 Number of feature selection  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stephanie, we did already a few models for feature selection and now we decide on the number, is this ok like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with 46 maximum features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats=46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first fit a linear model to the initial dataset to have a baseline to evaluate the data cleaning and feature engineering impact.\n",
    "\n",
    "To facilitate the training process, we will use the `sklearn` library <https://scikit-learn.org/stable/index.html> that provides a wrapper for the preprocessing, training, and evaluation of many machine learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lm_mod = linear_model.LogisticRegression(multi_class='multinomial',solver='saga',\n",
    "   max_iter=1000, penalty='none',n_jobs=-1)\n",
    "\n",
    "#initial_lm_mod = RandomForestRegressor(n_estimators=150)\n",
    "baseline_acc = np.mean(\n",
    "    cross_val_score(initial_lm_mod, X_train,y_train, cv=5))\n",
    "\n",
    "print(f\"Baseline model with Accuracy = {baseline_acc:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(clf, feature_names):\n",
    "    \"\"\"\n",
    "    Function to print the most important features of a logreg classifier\n",
    "    based on the coefficient values\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            'variable': feature_names, # Feature names\n",
    "            'coefficient': clf.coef_[0] # Feature Coeficients\n",
    "        }\n",
    "    ) \\\n",
    "    .round(decimals=2) \\\n",
    "    .sort_values('coefficient', ascending=False) \\\n",
    "    .style.bar(color=['red', 'green'], align='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_feature_importance(\n",
    "    initial_lm_mod.fit(X_val,y_val), \n",
    "    X_train.columns.get_level_values(0).tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get another view of feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.4.2'></a>\n",
    "## <font color=green> Embedded Method <font>\n",
    "## <font color=green> 6.4.2 Lasso Regularization <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying regularization to a Machine Learning model, we add a penalty to the model parameters to avoid that our model tries to resemble too closely our input data. In this way, we can make our model less complex and we can avoid overfitting (making learn to our model, not just the key data characteristics but also it’s intrinsic noise).\n",
    "One of the possible Regularization Methods is Lasso (L1) Regression. When using Lasso Regression, the coefficients of the inputs features gets shrunken if they are not positively contributing to our Machine Learning model training. In this way, some of the features might get automatically discarded assigning them coefficients equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_mod = linear_model.LogisticRegression(penalty='l1', solver='liblinear')\n",
    "print(\"Accuracy = {:.4}\".format(np.mean(cross_val_score(lasso_mod, X_train, y_train, cv=5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_importance(lasso_mod.fit(X_train,y_train), X_train.columns.get_level_values(0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar performance w.r.t the un-regularized models. However, you can see how the feature coefficients are smaller than the original ones, due to the regularization.\n",
    "\n",
    "Both methods give quite a lot of importance to elevation related variables\n",
    "\n",
    "\n",
    "Let's look at how the coefficient weights and accuracy scores change along with the different regularization values.\n",
    "To that end, I have implemented the following piece of code. Do not be overwhelmed by it. It basically defines a list of regularization values to test and train a new Logistic Regression model for one of these regularization values. We keep track of the coefficient values and the accuracy of each of these models to plot them according to the defined regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Watch out, takes 10 min to run on an i7 processor\n",
    "lasso_mod = linear_model.LogisticRegression(penalty='l1',solver='liblinear')\n",
    "alphas = 10**np.linspace(-1,-4,100)\n",
    "\n",
    "coefs_ = []\n",
    "scores_ = []\n",
    "for a in alphas:\n",
    "    lasso_mod.set_params(C=a)\n",
    "    scores_.append(np.mean(cross_val_score(lasso_mod, X_train, y_train, cv=5))) # Appends the accuracy of the model\n",
    "    coefs_.append(lasso_mod.fit(X_train, y_train).coef_.ravel().copy()) # Appends the coefficient of the model\n",
    "\n",
    "coefs_ = np.array(coefs_)\n",
    "scores_ = np.array(scores_)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "fig.suptitle('Logistic Regression Path', fontsize=20)\n",
    "\n",
    "# Coeff Weights Plot\n",
    "ax1.plot(alphas, coefs_, marker='o')\n",
    "ymin, ymax = plt.ylim()\n",
    "ax1.set_ylabel('Coefficient Weights', fontsize = 15)\n",
    "ax1.set_xlabel('log(C)', fontsize = 15)\n",
    "ax1.axis('tight')\n",
    "\n",
    "# Accuracy Plot\n",
    "ax2.plot(alphas, scores_, marker='o')\n",
    "ymin, ymax = plt.ylim()\n",
    "ax2.set_ylabel('Accuracy Score', fontsize = 15)\n",
    "ax2.set_xlabel('log(C)', fontsize = 15)\n",
    "ax2.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stephanie, is this explanation still in line with the graph above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the left figure, the smaller the alpha value (alpha), the larger the regularization and, consequently, the smaller the weights of the coefficients. This is because, if we check the sklearn documentation, we will see that this value is the: \"Inverse of regularization strength.\"\n",
    "\n",
    "When regularization is large enough (i.e., alpha is small), the values of the coefficients are close to 0 (i.e., null model).\n",
    "\n",
    "As there is a trade-off between variance (i.e., less over-fitted model --> more regularization) and bias (i.e., learning more from the training set --> less regularization), You must find the optimal alpha value. \n",
    "\n",
    "As you can see in the right figure, this value is achieved with small alpha values (i.e., more regularization). \n",
    "\n",
    "To automatize the process of finding the optimal value, you can make use of the LogisticRegressionCV function in sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) that performs CV, testing different hyperparameters (that you can provide) and selecting the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_lr_selector = SelectFromModel(LogisticRegression(C=0.04,penalty='l1',solver='liblinear',max_iter=1000,n_jobs=-1), max_features=num_feats)\n",
    "embeded_lr_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_lr_support = embeded_lr_selector.get_support()\n",
    "embeded_lr_feature = X_train.loc[:,embeded_lr_support].columns.tolist()\n",
    "print(str(len(embeded_lr_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_lr_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.4.3'></a>\n",
    "## <font color=green> 6.4.3 Filter Method <font>\n",
    "    \n",
    "<a id='6.4.3.1'></a>\n",
    "### <font color=green> 6.4.3.1 Anova F-value <font>\n",
    "Chi-Square does not work because it needs non - negative values. For that reason we will use Anova. It is a univariate filter method that uses variance to find out the separability of the individual features between classes. It applies to multi-class endpoints.The SelectKBest class just scores the features using a function  f_classif and then \"removes all but the k highest scoring features\n",
    "\n",
    "__f_classif__ =  ANOVA F-value between label/feature for classification tasks.\n",
    "    \n",
    "__k__ is the prior selected numbers of chosen features we want to select for further selection.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code from class Forum, select the best features \n",
    "anov_selector = SelectKBest(f_classif, k=num_feats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anov_support = anov_selector.get_support()\n",
    "# Get  columns from original dataframe\n",
    "anov_feature = X_train.iloc[:,anov_support].columns.tolist()\n",
    "print(str(len(anov_feature)), 'selected features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.4.3.2'></a>\n",
    "### <font color=green> 6.4.3.2. Pearson correlation <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get another set of features, this time giving more importance to the alpine climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = data_train['data'], data_train['Cover_Type']\n",
    "\n",
    "# Create a list of the feature names\n",
    "#features = np.array(data['feature_names'])\n",
    "fig, ax = plt.subplots(figsize=(10,40))         # Sample figsize in inches\n",
    "# Instantiate the visualizer\n",
    "visualizer = FeatureCorrelation(labels=None,sort=True)\n",
    "\n",
    "ax = visualizer.fit(X_train, y_train)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data_train.corrwith(data_train[\"Cover_Type\"])\n",
    "X_y = data_train.copy()\n",
    "X_y['Cover_Type'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,50))         # Sample figsize in inches\n",
    "\n",
    "corr_matrix = X_y.corr()\n",
    "\n",
    "# Isolate the column corresponding to `exam_score`\n",
    "corr_target = corr_matrix[['Cover_Type']].drop(labels=['Cover_Type'])\n",
    "corr_target_sorted = corr_target.sort_values(by = 'Cover_Type')\n",
    "sns.heatmap(corr_target_sorted, annot=True, fmt='.3', cmap='RdBu_r',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_selector(X, y,num_feats):\n",
    "    cor_list = []\n",
    "    feature_name = X.columns.tolist()\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    return cor_support, cor_feature\n",
    "cor_support, cor_feature = cor_selector(X_train, y_train,num_feats)\n",
    "print(str(len(cor_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.5'></a>\n",
    "## <font color=green> 6.5. Recursive Feature Elimination <font>\n",
    "\n",
    "The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rfe_selector = RFE(estimator=LogisticRegression(max_iter=3000), n_features_to_select=num_feats, step=10, verbose=5)\n",
    "rfe_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_support = rfe_selector.get_support()\n",
    "rfe_feature = X_train.loc[:,rfe_support].columns.tolist()\n",
    "print(str(len(rfe_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Watch out, it takes 20 min to run n an i7 processor\n",
    "cv = StratifiedKFold(5)\n",
    "visualizer = rfecv(LogisticRegression(max_iter=3000), X=X_train, y=y_train, cv=cv, scoring='accuracy', n_jobs=-1) #uses all processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.6'></a>\n",
    "## <font color=green> 6.6. Tree-based: SelectFromModel <font>\n",
    "<a id='6.6.1'></a>\n",
    "### <font color=green> 6.6.1 RandomForestClassifier<font>\n",
    "Embedded methods use algorithms that have built-in feature selection methods. We can also use RandomForest to select features based on feature importance. We calculate feature importance using node impurities in each decision tree. In Random forest, the final feature importance is the average of all decision tree feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I think I did this again a bit earlier, feel free to delete either mine or yours; or maybe we can use for comparison if this is post RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats)\n",
    "embeded_rf_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = X_train.loc[:,embeded_rf_support].columns.tolist()\n",
    "print(str(len(embeded_rf_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_rf_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.6.2'></a>\n",
    "### <font color=green> 6.6.2 XgBoost <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is relatively straightforward to retrieve importance scores for each attribute.\n",
    "\n",
    "Generally, importance provides a score that indicates how useful or valuable each feature was in the construction of the boosted decision trees within the model. The more an attribute is used to make key decisions with decision trees, the higher its relative importance.This importance is calculated explicitly for each attribute in the dataset, allowing attributes to be ranked and compared to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train needs to be transformed from 1,2,3,4,5,6,7 to 0 1 2 3 4 5,6\n",
    "le = LabelEncoder()\n",
    "y_train1 = le.fit_transform(y_train)\n",
    "\n",
    "model=xgb.XGBClassifier(learning_rate=0.1,n_estimators = 400,max_depth = 3,n_jobs=-1)\n",
    "\n",
    "embeded_xgb_selector = SelectFromModel(model, max_features=num_feats)\n",
    "embeded_xgb_selector.fit(X_train, y_train1)\n",
    "#learning_rate=0.1,n_estimators = 400,max_depth = 3,n_jobs=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We didn't use CV for XGBoost as he said it is quite sensible to hyperparameters, given that it takes up to 90 min to run and is just for feature selection I think we can leave it for the part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_xgb_support = embeded_xgb_selector.get_support()\n",
    "embeded_xgb_feature = X_train.loc[:,embeded_xgb_support].columns.tolist()\n",
    "print(str(len(embeded_xgb_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_xgb_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stephanie, is this repeated? You used it earlier as well, unless we are now rerunning after RFE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.6.3'></a>\n",
    "### <font color=green> 6.6.3 ExtraTreesClassifier <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Decision Tree in the Extra Trees Forest is constructed from the original training sample. Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.\n",
    "To perform feature selection using the above forest structure, during the construction of the forest, for each feature, the normalized total reduction in the mathematical criteria used in the decision of feature of split (Gini Index if the Gini Index is used in the construction of the forest) is computed. This value is called the Gini Importance of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tree_model= ExtraTreesClassifier()\n",
    "# Training the model\n",
    "extra_tree_forest_selector = SelectFromModel(ExtraTreesClassifier(max_features=num_feats,criterion='gini',n_jobs=-1))\n",
    "extra_tree_forest_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "extra_tree_model.fit(X_train,y_train)\n",
    "extra_tree_model.feature_importances_\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(extra_tree_model.feature_importances_, index=X_train.columns)\n",
    "ax = feat_importances.nlargest(num_feats).plot(kind='barh', colormap = 'rainbow')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tree_forest_support = extra_tree_forest_selector.get_support()\n",
    "extra_tree_forest_feature = X_train.loc[:,extra_tree_forest_support].columns.tolist()\n",
    "print(str(len(extra_tree_forest_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tree_forest_feature"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "chi2_score = pd.DataFrame(list(zip(X_train.columns, extra_tree_forest_support.scores_,extra_tree_forest_selector.pvalues_)), columns = ['feature','score','pvalue'])\n",
    "chi2_score.sort_values('score', ascending = False)\n",
    "\n",
    "chi2_score.style.set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.7'></a>\n",
    "## <font color=green> 6.7. Score of all methods together  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "# put all selection together\n",
    "feature_selection_df = pd.DataFrame({'Feature':feature_name,'ExtraTree':extra_tree_forest_support, 'Pearson':cor_support, 'RFE':rfe_support,'Anova':anov_support, 'Logistics':embeded_lr_support,\n",
    "                                    'Random Forest':embeded_rf_support, 'XGB':embeded_xgb_support})\n",
    "# count the selected times for each feature\n",
    "feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "# display the top 100\n",
    "feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "feature_selection_df.head(num_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression \n",
    "Esemble Learning and Random Forest \n",
    "Decision Trees \n",
    "Random Forest (XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [(feature_selection_df['Total']>4)]\n",
    " \n",
    "new_df = feature_selection_df.loc[feature_selection_df['Total'] >4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = new_df['Feature'].to_list()\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Since we want to add all the features of the dummy variale family and climate, we will remove them first. we also remove complex and soil for the time being and see how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = [column for column in feature_list if 'Soil' not in column and 'Type' not in  column and 'complex' not in  column and 'Family' not in column and 'Climate' not in column ]\n",
    "new\n",
    "missing = (data_train.filter(regex='Family').columns)\n",
    "missing2 = (data_train.filter(regex='Climate').columns)\n",
    "\n",
    "new.extend(missing)\n",
    "new.extend(missing2)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_xgb_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trial=data_train[embeded_xgb_feature]\n",
    "y_trial = data_train['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_trial.shape)\n",
    "y_trial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only X_Train replacement\n",
    "X_trial.to_csv('X_trial.csv')\n",
    "y_trial.to_csv('y_trial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = data_train[feature_list]\n",
    "y_selected = data_train['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only X_Train replacement\n",
    "X_selected.to_csv('X_selected.csv')\n",
    "y_selected.to_csv('y_selected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected1 = pd.read_csv(\"X_selected.csv\")\n",
    "y_selected1 = pd.read_csv(\"y_selected.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_selected1 = X_selected[X_selected.columns.drop(list(X_selected.filter(regex='Unnamed:')))]\n",
    "y_selected1 = y_selected[y_selected.columns.drop(list(y_selected.filter(regex='Unnamed:')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_selected.shape)\n",
    "print(y_selected.shape)\n",
    "\n",
    "if X.shape[0] != y.shape[0]:\n",
    "  print(\"X and y rows are mismatched, check dataset again\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "column_list = X_selected.columns\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_selected, y_selected, test_size=0.20, random_state=42,stratify=y)\n",
    "print(\"The shape of validation data:{} and {} \".format(X_val.shape,y_val.shape))\n",
    "print(\"The shape of training data:{} and {} \".format(X_train.shape,y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X_train_new,y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.score(X_val_new,y_val_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val_new, y_pred_test_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Correlation among features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap \n",
    "matrix = X_selected.corr()\n",
    "mask = np.zeros_like(matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "fig, ax = plt.subplots(figsize=(50,20))\n",
    "heatmap = sns.heatmap(matrix, center=0, fmt=\".3f\", square=True, annot=True, linewidth=1.3, mask = mask,vmax=0.9);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I go to sleep, see you in the whatsapp chat tomorrow :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feel free to delete all my markups, I ordered the code, added headings, ordered the index, ordered the imports and ran it from end to end. Seems to be working and functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
